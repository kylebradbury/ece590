{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5 - Reinforcement Learning\n",
    "\n",
    "## *YOUR FULL NAME HERE*\n",
    "Netid:  *Your netid here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Blackjack\n",
    "\n",
    "Your goal is to develop a reinforcement learning technique to learn the optimal policy for winning at blackjack. Here, we're going to modify the rules from traditional blackjack a bit in a way that corresponds to the game presented in Sutton and Barto's *Reinforcement Learning: An Introduction* (Chapter 5, example 5.1). A full implementation of the game is provided and usage examples are detailed in the class header below.\n",
    "\n",
    "The rules of this modified version of the game of blackjack are as follows:\n",
    "\n",
    "- Blackjack is a card game where the goal is to obtain cards that sum to as near as possible to 21 without going over.  We're playing against a fixed (autonomous) dealer.\n",
    "- Face cards (Jack, Queen, King) have point value 10. Aces can either count as 11 or 1, and we're refer to it as 'usable' at 11 (indicating that it could be used as a '1' if need be. This game is placed with a deck of cards sampled with replacement.\n",
    "- The game starts with each (player and dealer) having one face up and one face down card.\n",
    "- The player can request additional cards (hit, or action '1') until they decide to stop (stay, action '0') or exceed 21 (bust, the game ends and player loses).\n",
    "- After the player stays, the dealer reveals their facedown card, and draws until their sum is 17 or greater. If the dealer goes bust the player wins. If neither player nor dealer busts, the outcome (win, lose, draw) is decided by whose sum is closer to 21.  The reward for winning is +1, drawing is 0, and losing is -1.\n",
    "\n",
    "\n",
    "\n",
    "You will accomplish three things:\n",
    "1. Try your hand at this game of blackjack and see what your human reinforcement learning system is able to achieve\n",
    "2. Evaluate a simple policy using Monte Carlo policy evaluation\n",
    "3. Determine an optimal policy using Monte Carlo control\n",
    "\n",
    "*This problem is adapted from David Silver's [excellent series on Reinforcement Learning](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html) at University College London*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "\n",
    "### [10 points] Human reinforcement learning \n",
    "\n",
    "Using the code detailed below, play 50 hands of blackjack, and record your overall average reward. This will help you get accustomed with how the game works, the data structures involved with representing states, and what strategies are most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class Blackjack():\n",
    "    \"\"\"Simple blackjack environment adapted from OpenAI Gym:\n",
    "        https://github.com/openai/gym/blob/master/gym/envs/toy_text/blackjack.py\n",
    "    \n",
    "    Blackjack is a card game where the goal is to obtain cards that sum to as\n",
    "    near as possible to 21 without going over.  They're playing against a fixed\n",
    "    dealer.\n",
    "    \n",
    "    Face cards (Jack, Queen, King) have point value 10.\n",
    "    Aces can either count as 11 or 1, and it's called 'usable' at 11.\n",
    "    This game is placed with a deck sampled with replacement.\n",
    "    \n",
    "    The game starts with each (player and dealer) having one face up and one\n",
    "    face down card.\n",
    "    \n",
    "    The player can request additional cards (hit = 1) until they decide to stop\n",
    "    (stay = 0) or exceed 21 (bust).\n",
    "    \n",
    "    After the player stays, the dealer reveals their facedown card, and draws\n",
    "    until their sum is 17 or greater.  If the dealer goes bust the player wins.\n",
    "    If neither player nor dealer busts, the outcome (win, lose, draw) is\n",
    "    decided by whose sum is closer to 21.  The reward for winning is +1,\n",
    "    drawing is 0, and losing is -1.\n",
    "    \n",
    "    The observation is a 3-tuple of: the players current sum,\n",
    "    the dealer's one showing card (1-10 where 1 is ace),\n",
    "    and whether or not the player holds a usable ace (0 or 1).\n",
    "    \n",
    "    This environment corresponds to the version of the blackjack problem\n",
    "    described in Example 5.1 in Reinforcement Learning: An Introduction\n",
    "    by Sutton and Barto (1998).\n",
    "    \n",
    "    http://incompleteideas.net/sutton/book/the-book.html\n",
    "    \n",
    "    Usage: \n",
    "        Initialize the class:\n",
    "            game = Blackjack()\n",
    "        \n",
    "        Deal the cards:\n",
    "            game.deal()\n",
    "            \n",
    "             (14, 3, False)\n",
    "             \n",
    "            This is the agent's observation of the state of the game:\n",
    "            The first value is the sum of cards in your hand (14 in this case)\n",
    "            The second is the visible card in the dealer's hand (3 in this case)\n",
    "            The Boolean is a flag (False in this case) to indicate whether or \n",
    "                not you have a usable Ace\n",
    "            (Note: if you have a usable ace, the sum will treat the ace as a \n",
    "                value of '11' - this is the case if this Boolean flag is \"true\")\n",
    "            \n",
    "        Take an action: Hit (1) or stay (0)\n",
    "        \n",
    "            Take a hit: game.step(1)\n",
    "            To Stay:    game.step(0)\n",
    "            \n",
    "        The output summarizes the game status:\n",
    "            \n",
    "            ((15, 3, False), 0, False)\n",
    "            \n",
    "            The first tuple (15, 3, False), is the agent's observation of the\n",
    "            state of the game as described above.\n",
    "            The second value (0) indicates the rewards\n",
    "            The third value (False) indicates whether the game is finished\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 1 = Ace, 2-10 = Number cards, Jack/Queen/King = 10\n",
    "        self.deck   = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 10, 10]\n",
    "        self.dealer = []\n",
    "        self.player = []\n",
    "        self.deal()\n",
    "\n",
    "    def step(self, action):\n",
    "        if action == 1:  # hit: add a card to players hand and return\n",
    "            self.player.append(self.draw_card())\n",
    "            if self.is_bust(self.player):\n",
    "                done = True\n",
    "                reward = -1\n",
    "            else:\n",
    "                done = False\n",
    "                reward = 0\n",
    "        else:  # stay: play out the dealers hand, and score\n",
    "            done = True\n",
    "            while self.sum_hand(self.dealer) < 17:\n",
    "                self.dealer.append(self.draw_card())\n",
    "            reward = self.cmp(self.score(self.player), self.score(self.dealer))\n",
    "        return self._get_obs(), reward, done\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return (self.sum_hand(self.player), self.dealer[0], self.usable_ace(self.player))\n",
    "\n",
    "    def deal(self):\n",
    "        self.dealer = self.draw_hand()\n",
    "        self.player = self.draw_hand()\n",
    "        return self._get_obs()\n",
    "    \n",
    "    #------------------------------------------\n",
    "    # Other helper functions\n",
    "    #------------------------------------------\n",
    "    def cmp(self, a, b):\n",
    "        return float(a > b) - float(a < b)\n",
    "    \n",
    "    def draw_card(self):\n",
    "        return int(np.random.choice(self.deck))\n",
    "    \n",
    "    def draw_hand(self):\n",
    "        return [self.draw_card(), self.draw_card()]\n",
    "    \n",
    "    def usable_ace(self,hand):  # Does this hand have a usable ace?\n",
    "        return 1 in hand and sum(hand) + 10 <= 21\n",
    "    \n",
    "    def sum_hand(self,hand):  # Return current hand total\n",
    "        if self.usable_ace(hand):\n",
    "            return sum(hand) + 10\n",
    "        return sum(hand)\n",
    "    \n",
    "    def is_bust(self,hand):  # Is this hand a bust?\n",
    "        return self.sum_hand(hand) > 21\n",
    "    \n",
    "    def score(self,hand):  # What is the score of this hand (0 if bust)\n",
    "        return 0 if self.is_bust(hand) else self.sum_hand(hand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of how it works to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 10, False)\n",
      "((22, 10, False), -1, True)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize the class:\n",
    "game = Blackjack()\n",
    "\n",
    "# Deal the cards:\n",
    "s0 = game.deal()\n",
    "print(s0)\n",
    "\n",
    "# Take an action: Hit = 1 or stay = 0. Here's a hit:\n",
    "s1 = game.step(1)\n",
    "print(s1)\n",
    "\n",
    "# If you wanted to stay:\n",
    "# game.step(2)\n",
    "\n",
    "# When it's gameover, just redeal:\n",
    "# game.deal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2\n",
    "\n",
    "### [40 points] Perform Monte Carlo Policy Evaluation\n",
    "\n",
    "Thinking that you want to make your millions playing blackjack, you decide to test out a policy for playing this game. Your idea is an aggressive strategy: always hit unless the total of your cards adds up to 20 or 21, in which case you stay.\n",
    "\n",
    "**(a)** Use Monte Carlo policy evaluation to evaluate the expected returns from each state. Create plots for these similar to Sutton and Barto, Figure 5.1 where you plot the expected returns for each state. In this case create 2 plots:\n",
    "1. When you have a useable ace, plot the state space with the dealer's card on the x-axis, and the player's sum on the y-axis, and use the 'RdBu' matplotlib colormap and `imshow` to plot the value of each state under the policy described above. The domain of your x and y axes should include all possible states (2 to 21 for the player sum, and 1 to 10 for the dealer's card). Do this for for 10,000 episodes.\n",
    "2. Repeat (1) for the states without a usable ace.\n",
    "3. Repeat (1) for the case of 500,000 episodes.\n",
    "4. Relwat (2) for the case of 500,000 episodes.\n",
    "\n",
    "**(b)** Show a plot of the overall average reward per episode vs the number of episodes. For both the 10,000 episode case and the 500,000 episode case, record the overall average reward for this policy and report that value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "\n",
    "### [40 points] Perform Monte Carlo Control\n",
    "\n",
    "**(a)** Using Monte Carlo Control through policy iteration, estimate the optimal policy for playing our modified blackjack game to maximize rewards. \n",
    "\n",
    "In doing this, use the following assumptions:\n",
    "1. Initialize the value function and the state value function to all zeros\n",
    "2. Keep a running tally of the number of times the agent visited each state and chose an action. $N(s_t,a_t)$ is the number of times action $a$ has been selected from state $s$. You'll need this to compute the running average. You can implement an online average as: $\\bar{x}_{t} = \\frac{1}{N}x_t + \\frac{N-1}{N}\\bar{x}_{t-1}$ \n",
    "3. Use an $\\epsilon$-greedy exploration strategy with $\\epsilon_t = \\frac{N_0}{N_0 + N(s_t)}$, where we define $N_0 = 100$. Vary $N_0$ as needed.\n",
    "\n",
    "Show your result by plotting the optimal value function: $V^*(s) = max_a Q^*(s,a)$ and the optimal policy $\\pi^*(s)$. Create plots for these similar to Sutton and Barto, Figure 5.2 in the new draft edition, or 5.5 in the original edition. Your results SHOULD be very similar to the plots in that text. For these plots include:\n",
    "1. When you have a useable ace, plot the state space with the dealer's card on the x-axis, and the player's sum on the y-axis, and use the 'RdBu' matplotlib colormap and `imshow` to plot the value of each state under the policy described above. The domain of your x and y axes should include all possible states (2 to 21 for the player sum, and 1 to 10 for the dealer's visible card).\n",
    "2. Repeat (1) for the states without a usable ace.\n",
    "3. A plot of the optimal policy $\\pi^*(s)$ for the states with a usable ace (this plot could be an imshow plot with binary values).\n",
    "4. A plot of the optimal policy $\\pi^*(s)$ for the states without a usable ace (this plot could be an imshow plot with binary values).\n",
    "\n",
    "**(b)** Show a plot of the overall average reward per episode vs the number of episodes. What is the average reward your control strategy was able to achieve?\n",
    "\n",
    "*Note: convergence of this algorithm is extremely slow. You may need to let this run a few million episodes before the policy starts to converge. You're not expected to get EXACTLY the optimal policy, but it should be visibly close.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "### [10 points] Discuss your findings\n",
    "\n",
    "Compare the performance of your human control policy, the naive policy from question 2, and the optimal control policy in question 3. \n",
    "**(a)** Which performs best? Why is this the case? \n",
    "**(b)** Could you have created a better policy if you knew the full Markov Decision Process for this environment? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANSWER**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
